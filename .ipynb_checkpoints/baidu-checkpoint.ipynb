{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baidu.ipynb\t    dev_deleted.csv\t    train_data_frame.csv\r\n",
      "bert_config\t    final_test_data.csv     train_deleted.csv\r\n",
      "dev_data_baidu.csv  test_data_with_stp.csv  Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EPOCHS = 6\n",
    "PATH = \"bert_config\" #！！！需要改 config所在为止 我用的预训练模型注意下！！！\n",
    "TRAINING_FILE = \"train_data_frame.csv\" #！！！需要改\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def process_data(text, question, answer, tokenizer, max_len):\\n    len_ans = len(answer)\\n    idx0 = 0\\n    idx1 = 0\\n    for index in (i for i, e in enumerate(text) if e == answer[0]):\\n        if text[index:index+len_ans] == answer:\\n            idx0 = index\\n            idx1 = index + len_ans - 1\\n            break\\n   \\n    ans_start = 0\\n    ans_end = 0\\n    posslen = max_len - len(question) - 3\\n    if idx0 != 0:\\n        ans_start = idx0+1\\n    if idx1 != 0:\\n        ans_end = idx1+1\\n    if ans_start > posslen:\\n        ans_start = 511\\n    if ans_end > posslen:\\n        ans_end = 511\\n        \\n    if len(text) > posslen:\\n        text = text[:posslen]\\n        token_text = tokenizer.encode_plus(text,question)\\n        input_ids_orig = token_text[\"input_ids\"]\\n    else:\\n        token_text = tokenizer.encode_plus(text,question)\\n        input_ids_orig = token_text[\"input_ids\"]\\n        \\n    input_ids = input_ids_orig\\n    #input_ids = [2] + input_ids_orig + [2]\\n    token_type_ids = [0] * (len(input_ids_orig))\\n    #token_type_ids = [0] + [0] * (len(input_ids_orig) + 1)\\n    mask = [1] * len(token_type_ids)\\n    #ans_start += 1\\n    #ans_end +=1\\n    \\n    \\n    padding_length = max_len - len(input_ids)\\n    if padding_length > 0:\\n        input_ids = input_ids + ([1] * padding_length)\\n        mask = mask + [0] * padding_length\\n        token_type_ids = token_type_ids + [0] * padding_length\\n        \\n    return {\\n        \\'ids\\': input_ids,\\n        \\'mask\\': mask,\\n        \\'token_type_ids\\': token_type_ids,\\n        \\'ans_start\\': ans_start,\\n        \\'ans_end\\': ans_end,\\n        \\'orig_text\\': text,\\n        \\'orig_ans\\': answer\\n    }'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#不需要了\n",
    "\"\"\"def process_data(text, question, answer, tokenizer, max_len):\n",
    "    len_ans = len(answer)\n",
    "    idx0 = 0\n",
    "    idx1 = 0\n",
    "    for index in (i for i, e in enumerate(text) if e == answer[0]):\n",
    "        if text[index:index+len_ans] == answer:\n",
    "            idx0 = index\n",
    "            idx1 = index + len_ans - 1\n",
    "            break\n",
    "   \n",
    "    ans_start = 0\n",
    "    ans_end = 0\n",
    "    posslen = max_len - len(question) - 3\n",
    "    if idx0 != 0:\n",
    "        ans_start = idx0+1\n",
    "    if idx1 != 0:\n",
    "        ans_end = idx1+1\n",
    "    if ans_start > posslen:\n",
    "        ans_start = 511\n",
    "    if ans_end > posslen:\n",
    "        ans_end = 511\n",
    "        \n",
    "    if len(text) > posslen:\n",
    "        text = text[:posslen]\n",
    "        token_text = tokenizer.encode_plus(text,question)\n",
    "        input_ids_orig = token_text[\"input_ids\"]\n",
    "    else:\n",
    "        token_text = tokenizer.encode_plus(text,question)\n",
    "        input_ids_orig = token_text[\"input_ids\"]\n",
    "        \n",
    "    input_ids = input_ids_orig\n",
    "    #input_ids = [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0] * (len(input_ids_orig))\n",
    "    #token_type_ids = [0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    #ans_start += 1\n",
    "    #ans_end +=1\n",
    "    \n",
    "    \n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + [0] * padding_length\n",
    "        token_type_ids = token_type_ids + [0] * padding_length\n",
    "        \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'ans_start': ans_start,\n",
    "        'ans_end': ans_end,\n",
    "        'orig_text': text,\n",
    "        'orig_ans': answer\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到完全匹配的正确答案\n",
    "def find_ans(text, answer):\n",
    "    for index in (i for i, e in enumerate(text) if e == answer[0]):\n",
    "            if text[index:index+len(answer)] == answer:\n",
    "                idx0 = index\n",
    "                idx1 = index + len(answer)\n",
    "                return idx0, idx1\n",
    "    return find_start(text, answer[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def find_left(text, text_l, count):\\n    for i in text:\\n        if i == \"，\" or i == \"。\":\\n            text_l += 1\\n            if text_l < count:\\n                return find_left(text[count_l:], text_l, count)\\n            else:\\n                return text_l \\n        else:\\n            text_l += 1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#不需要了\n",
    "\"\"\"def find_left(text, text_l, count):\n",
    "    for i in text:\n",
    "        if i == \"，\" or i == \"。\":\n",
    "            text_l += 1\n",
    "            if text_l < count:\n",
    "                return find_left(text[count_l:], text_l, count)\n",
    "            else:\n",
    "                return text_l \n",
    "        else:\n",
    "            text_l += 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#截取答案所在句为止的512长度text\n",
    "def find_text(ans_start, ans_end, text, posslen):\n",
    "    text_l = ans_start\n",
    "    text_r = ans_end\n",
    "    count = 0\n",
    "    for i in text[ans_end:]:\n",
    "        if i==\"。\":\n",
    "            text_r += 1\n",
    "            break\n",
    "        else:\n",
    "            text_r += 1\n",
    "    text_l = text_r - posslen\n",
    "    return text_l, text_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(text, question, answer, tokenizer, max_len):\n",
    "    len_ans = len(answer)\n",
    "    posslen = max_len - len(question) - 3\n",
    "    \n",
    "    ans_start, ans_end = find_ans(text, answer)\n",
    "    text_l, text_r = find_text(ans_start, ans_end, text, posslen)\n",
    "    if len(text) > posslen:\n",
    "        text = text[text_l:text_r]\n",
    "    token_text = tokenizer.encode_plus(text,question)\n",
    "    input_ids_orig = token_text[\"input_ids\"]\n",
    "        \n",
    "    input_ids = input_ids_orig\n",
    "    token_type_ids = [0] * (len(input_ids_orig))\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    ans_start += 1\n",
    "    ans_end += 1\n",
    "    \n",
    "    \n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + [0] * padding_length\n",
    "        token_type_ids = token_type_ids + [0] * padding_length\n",
    "        \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'ans_start': ans_start,\n",
    "        'ans_end': ans_end,\n",
    "        'orig_text': text,\n",
    "        'orig_ans': answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAdataset:\n",
    "    def __init__(self, text, question, answer):\n",
    "        self.text = text\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.text[item],\n",
    "            self.question[item],\n",
    "            self.answer[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "            )\n",
    "        \n",
    "        return{\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'ans_start': torch.tensor(data[\"ans_start\"], dtype=torch.long),\n",
    "            'ans_end': torch.tensor(data[\"ans_end\"], dtype=torch.long),\n",
    "            'orig_text': data[\"orig_text\"],\n",
    "            'orig_ans': data[\"orig_ans\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAmodel(transformers.BertForQuestionAnswering):\n",
    "    def __init__(self, conf):\n",
    "        super(QAmodel,self).__init__(conf)\n",
    "        self.bertwwm = transformers.BertModel.from_pretrained(\"hfl/chinese-bert-wwm-ext\", config=conf)\n",
    "        self.drop_out = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.out.weight, std=0.02)\n",
    "        \n",
    "    def forward(self, input_ids, mask, token_type_ids):\n",
    "        _, _, out = self.bertwwm(input_ids, token_type_ids=token_type_ids, attention_mask=mask)\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        logits = self.out(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        ans_start = d[\"ans_start\"]\n",
    "        ans_end = d[\"ans_end\"]\n",
    "        orig_ans = d[\"orig_ans\"]\n",
    "        orig_text = d[\"orig_text\"]\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        ans_start = ans_start.to(device, dtype=torch.long)\n",
    "        ans_end = ans_end.to(device, dtype=torch.long)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs_start, outputs_end = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs_start, outputs_end, ans_start, ans_end)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "transform: failed to synchronize: cudaErrorAssert: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-00852b39e007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"over epoch-------------------------\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#torch.save(model, 'QAdev.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c6e659c61c55>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TFboys/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TFboys/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: transform: failed to synchronize: cudaErrorAssert: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE)\n",
    "    \n",
    "train_dataset = QAdataset(\n",
    "    text=df_train.context.values,\n",
    "    question=df_train.question.values,\n",
    "    answer=df_train.answer.values\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "\n",
    "\"\"\"valid_dataset = QAdataset(\n",
    "    text=df_valid.text.values,\n",
    "    question=df_vaild.question.values,\n",
    "    answer=df_valid.answer.values\n",
    ")\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    ")\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda:1\") #!!!需要改成\"cuda\"\n",
    "\n",
    "  \n",
    "model_config = transformers.BertConfig.from_pretrained(PATH)\n",
    "\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "model = QAmodel(conf=model_config)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},#if not any(nd in n for nd in no_decay)\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}#if any(nd in n for nd in no_decay)\n",
    "]\n",
    "optimizer = AdamW(optimizer_parameters, lr=2e-5)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "for epoch in range(6):\n",
    " \n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "    print(\"over epoch-------------------------\", epoch)\n",
    "#torch.save(model, 'QAdev.pt')   \n",
    "torch.save(model.state_dict(), 'QAdev_params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评价模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model=torch.load(\"QAdev.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datavalid(text, question, tokenizer, max_len):\n",
    "            \n",
    "    token_text = tokenizer.encode_plus(text,question)\n",
    "    input_ids_orig = token_text[\"input_ids\"]\n",
    "\n",
    "\n",
    "    input_ids = input_ids_orig\n",
    "    token_type_ids = [0] * (len(input_ids_orig))\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    \n",
    "    posslen = max_len - len(question) - 3\n",
    "    if len(text) > posslen:\n",
    "        text = text[:posslen]\n",
    "    token_text = tokenizer.encode_plus(text,question)\n",
    "    input_ids_orig = token_text[\"input_ids\"]\n",
    "\n",
    "    input_ids = input_ids_orig\n",
    "    token_type_ids = [0] * (len(input_ids_orig))\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    \n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + [0] * padding_length\n",
    "        token_type_ids = token_type_ids + [0] * padding_length\n",
    "\n",
    "        \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'orig_text': text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAdatasetvalid:\n",
    "    def __init__(self, text,question):\n",
    "        self.text = text\n",
    "        self.question = question\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = process_datavalid(\n",
    "            self.text[item],\n",
    "            self.question[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "            )\n",
    "        \n",
    "        return{\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'orig_text': data[\"orig_text\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_FILE = 'test.csv'\n",
    "df_valid = pd.read_csv(VALID_FILE)\n",
    "valid_dataset = QAdatasetvalid(\n",
    "    text=df_valid.text.values,#\n",
    "    question=df_valid.qeuestion.values,\n",
    ")\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluation function to predict on the test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    " \n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            orig_text = d[\"orig_text\"]\n",
    "            \n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs_start, outputs_end = model(\n",
    "                ids,\n",
    "                mask,\n",
    "                token_type_ids\n",
    "            )\n",
    "\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            idx_start=np.argmax(outputs_start)\n",
    "            idx_end=np.argmax(outputs_end)\n",
    "            out = orig_text[0][int(idx_start)-1:int(idx_end)-1]\n",
    "            with open(\"res.txt\",\"a\")as w:\n",
    "                w.write(out)\n",
    "                w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad685023affb48b2b6aac8a17c7be943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\") #!!!需要改成\"cuda\"\n",
    "\n",
    "aaa = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "eval_fn(valid_data_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
